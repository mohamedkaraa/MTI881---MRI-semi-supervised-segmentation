{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Dice\n",
    "\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "from losses import *\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from UNet_Attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining():\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 8\n",
    "    batch_size_val = 8\n",
    "    lr = 0.001     # Learning Rate\n",
    "    weight_decay = 1e-5\n",
    "    epoch = 200 # Number of epochs\n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=False,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                            batch_size=batch_size,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = 'Att_UNet_Model'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    # net = AttU_Net(num_classes)\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss()\n",
    "    dice_loss = DiceLoss(num_classes)\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        CE_loss.cuda()\n",
    "        dice_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalValidation = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "\n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "\n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        vlossEpoch = []\n",
    "        DSCEpoch = []\n",
    "        vDSCEpoch = []\n",
    "        DSCEpoch_w = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        v_num_batches = len(val_loader)\n",
    "        ########## Training ##########\n",
    "        net.train(True)\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net.forward(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            # COMPUTE THE LOSS\n",
    "            CE_loss_value = CE_loss(softMax(net_predictions),segmentation_classes)\n",
    "            dice_loss_value = dice_loss(softMax(net_predictions),segmentation_classes.unsqueeze(1))\n",
    "            lossTotal = 0.5*(CE_loss_value + dice_loss_value)\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(CE_loss_value.cpu().data.numpy())\n",
    "            DSCEpoch.append(dice_loss_value.cpu().data.numpy())\n",
    "            print\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" CE_Loss: {:.4f}, dice_loss:  {:.4f}\".format(CE_loss_value,dice_loss_value))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "        DSCEpoch = np.asarray(DSCEpoch)\n",
    "        DSCEpoch = DSCEpoch.mean()\n",
    "        lossTotalTraining.append(lossEpoch+DSCEpoch)\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                                done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch+DSCEpoch))\n",
    "        \n",
    "        ######### Validation ############\n",
    "        net.train(False)\n",
    "        for j, vdata in enumerate(val_loader):\n",
    "            vimages, vlabels, vimg_names = vdata\n",
    "            vlabels = to_var(vlabels)\n",
    "            vimages = to_var(vimages)\n",
    "            voutputs = net(vimages)\n",
    "            vsegmentation_classes = getTargetSegmentation(vlabels)\n",
    "            vloss = CE_loss(softMax(voutputs), vsegmentation_classes)\n",
    "            vdice = dice_loss(softMax(voutputs), vsegmentation_classes.unsqueeze(1))\n",
    "            vlossTotal = 0.5*(vloss + vdice)\n",
    "            vlossEpoch.append(vloss.cpu().data.numpy())\n",
    "            vDSCEpoch.append(vdice.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" CE_val_Loss: {:.4f}, dice_val_loss: {:.4f}\".format(vloss,vdice))\n",
    "            \n",
    "        vlossEpoch = np.asarray(vlossEpoch)\n",
    "        vlossEpoch = vlossEpoch.mean()\n",
    "        vDSCEpoch = np.asarray(vDSCEpoch)\n",
    "        vDSCEpoch = vDSCEpoch.mean()\n",
    "        lossTotalValidation.append(vlossEpoch+vDSCEpoch)\n",
    "        printProgressBar(v_num_batches, v_num_batches,\n",
    "                                done=\"[Validation] Epoch: {}, val_LossG: {:.4f}\".format(i,vlossEpoch+vDSCEpoch))\n",
    "\n",
    "\n",
    "\n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "            os.makedirs('./models/' + modelName)\n",
    "        if  vlossEpoch < Best_loss_val:\n",
    "            Best_loss_val = vlossEpoch\n",
    "            BestEpoch = i \n",
    "            torch.save(net.state_dict(), './models/' + modelName + '/'  + str(BestEpoch)+'_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'TrainLosses.npy'), lossTotalTraining)\n",
    "        np.save(os.path.join(directory, 'ValLosses.npy'), lossTotalValidation)\n",
    "\n",
    "runTraining()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingTversky():\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 8\n",
    "    batch_size_val = 8\n",
    "    lr = 0.0001     # Learning Rate\n",
    "    weight_decay = 1e-5\n",
    "    alpha = 0.3 # 0.5 0.3 0.8\n",
    "    beta = 0.8 # 0.5 0.8 0.3\n",
    "    epoch = 200 # Number of epochs\n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=False,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                            batch_size=batch_size,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = f'Tversky_Model_{alpha}_{beta}_att'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    # net = UNet(num_classes)\n",
    "    net = AttU_Net(num_classes)\n",
    "\n",
    "\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    tversky = TverskyLoss(alpha=alpha,beta=beta,n_classes=num_classes)\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        tversky.cuda()\n",
    "\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalValidation = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "\n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "\n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        vlossEpoch = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        v_num_batches = len(val_loader)\n",
    "        ########## Training ##########\n",
    "        net.train(True)\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net.forward(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            # COMPUTE THE LOSS\n",
    "            tversky_loss_value = tversky(softMax(net_predictions),segmentation_classes)\n",
    "            lossTotal = tversky_loss_value\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(tversky_loss_value.cpu().data.numpy())\n",
    "            print\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" Tversky_Loss: {:.4f}\".format(tversky_loss_value))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                                done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "        \n",
    "        ######### Validation ############\n",
    "        net.train(False)\n",
    "        for j, vdata in enumerate(val_loader):\n",
    "            vimages, vlabels, vimg_names = vdata\n",
    "            vlabels = to_var(vlabels)\n",
    "            vimages = to_var(vimages)\n",
    "            voutputs = net(vimages)\n",
    "            vsegmentation_classes = getTargetSegmentation(vlabels)\n",
    "            vloss = tversky(softMax(voutputs), vsegmentation_classes)\n",
    "            vlossTotal = vloss\n",
    "            vlossEpoch.append(vloss.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" Tversky_val_Loss: {:.4f}\".format(vloss))\n",
    "            \n",
    "        vlossEpoch = np.asarray(vlossEpoch)\n",
    "        vlossEpoch = vlossEpoch.mean()\n",
    "\n",
    "        lossTotalValidation.append(vlossEpoch)\n",
    "        printProgressBar(v_num_batches, v_num_batches,\n",
    "                                done=\"[Validation] Epoch: {}, val_LossG: {:.4f}\".format(i,vlossEpoch))\n",
    "\n",
    "\n",
    "\n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "            os.makedirs('./models/' + modelName)\n",
    "        if  vlossEpoch < Best_loss_val:\n",
    "            Best_loss_val = vlossEpoch\n",
    "            BestEpoch = i \n",
    "            torch.save(net.state_dict(), './models/' + modelName + '/'  + str(BestEpoch)+'_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'TrainLosses.npy'), lossTotalTraining)\n",
    "        np.save(os.path.join(directory, 'ValLosses.npy'), lossTotalValidation)\n",
    "\n",
    "runTrainingTversky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTrainingFocalTversky():\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 32\n",
    "    batch_size_val = 16\n",
    "    lr = 0.001     # Learning Rate\n",
    "    weight_decay = 1e-5\n",
    "    alpha = 0.3 # 0.5 0.3 0.8\n",
    "    beta = 0.8 # 0.5 0.8 0.3\n",
    "    gamma = 0.5\n",
    "    epoch = 200 # Number of epochs\n",
    "\n",
    "    root_dir = './Data/'\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=False,\n",
    "                                                    equalize=False)\n",
    "\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                            batch_size=batch_size,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=True)\n",
    "\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "    val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName = f'Tversky_Model_{alpha}_{beta}_focal'\n",
    "    print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "    ## CREATION OF YOUR MODEL\n",
    "    net = UNet(num_classes)\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad)))\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    tversky = FocalTverskyLoss(alpha=alpha,beta=beta,gamma=gamma,n_classes=num_classes)\n",
    "    ## PUT EVERYTHING IN GPU RESOURCES    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        tversky.cuda()\n",
    "\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalValidation = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "\n",
    "    directory = 'Results/Statistics/' + modelName\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    ## START THE TRAINING\n",
    "\n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(epoch):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        vlossEpoch = []\n",
    "        num_batches = len(train_loader_full)\n",
    "        v_num_batches = len(val_loader)\n",
    "        ########## Training ##########\n",
    "        net.train(True)\n",
    "        ## FOR EACH BATCH\n",
    "        for j, data in enumerate(train_loader_full):\n",
    "            ### Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            ### From numpy to torch variables\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            ################### Train ###################\n",
    "            #-- The CNN makes its predictions (forward pass)\n",
    "            net_predictions = net.forward(images)\n",
    "\n",
    "            #-- Compute the losses --#\n",
    "            # THIS FUNCTION IS TO CONVERT LABELS TO A FORMAT TO BE USED IN THIS CODE\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            # COMPUTE THE LOSS\n",
    "            tversky_loss_value = tversky(softMax(net_predictions),segmentation_classes)\n",
    "            lossTotal = tversky_loss_value\n",
    "\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            lossTotal.backward()\n",
    "            optimizer.step()\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING \n",
    "            lossEpoch.append(tversky_loss_value.cpu().data.numpy())\n",
    "            print\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Training] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" Tversky_Loss: {:.4f}\".format(tversky_loss_value))\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "        printProgressBar(num_batches, num_batches,\n",
    "                                done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(i,lossEpoch))\n",
    "        \n",
    "        ######### Validation ############\n",
    "        net.train(False)\n",
    "        for j, vdata in enumerate(val_loader):\n",
    "            vimages, vlabels, vimg_names = vdata\n",
    "            vlabels = to_var(vlabels)\n",
    "            vimages = to_var(vimages)\n",
    "            voutputs = net(vimages)\n",
    "            vsegmentation_classes = getTargetSegmentation(vlabels)\n",
    "            vloss = tversky(softMax(voutputs), vsegmentation_classes)\n",
    "            vlossTotal = vloss\n",
    "            vlossEpoch.append(vloss.cpu().data.numpy())\n",
    "            printProgressBar(j + 1, num_batches,\n",
    "                                prefix=\"[Validation] Epoch: {} \".format(i),\n",
    "                                length=15,\n",
    "                                suffix=\" Tversky_val_Loss: {:.4f}\".format(vloss))\n",
    "            \n",
    "        vlossEpoch = np.asarray(vlossEpoch)\n",
    "        vlossEpoch = vlossEpoch.mean()\n",
    "\n",
    "        lossTotalValidation.append(vlossEpoch)\n",
    "        printProgressBar(v_num_batches, v_num_batches,\n",
    "                                done=\"[Validation] Epoch: {}, val_LossG: {:.4f}\".format(i,vlossEpoch))\n",
    "\n",
    "\n",
    "\n",
    "        ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH. \n",
    "        ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "        if not os.path.exists('./models/' + modelName):\n",
    "            os.makedirs('./models/' + modelName)\n",
    "        if  vlossEpoch < Best_loss_val:\n",
    "            Best_loss_val = vlossEpoch\n",
    "            BestEpoch = i \n",
    "            torch.save(net.state_dict(), './models/' + modelName + '/'  + str(BestEpoch)+'_Epoch')\n",
    "            \n",
    "        np.save(os.path.join(directory, 'TrainLosses.npy'), lossTotalTraining)\n",
    "        np.save(os.path.join(directory, 'ValLosses.npy'), lossTotalValidation)\n",
    "\n",
    "runTrainingFocalTversky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3486f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(num_classes=4) # base unet\n",
    "net = AttU_Net(num_classes=4) # attention unet\n",
    "\n",
    "net.load_state_dict(torch.load(r'models\\Tversky_Model_0.3_0.8_att\\198_Epoch'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "test_image = cv2.imread(r\"Data/val/Img/patient012_01_10.png\",0)\n",
    "test_image = transform(test_image).float()\n",
    "test_image = test_image.unsqueeze(0)\n",
    "test_image_label = cv2.imread(r\"Data/val/GT/patient012_01_10.png\",0)\n",
    "test_image_label = transform(test_image_label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "softMax = torch.nn.Softmax(dim=1)\n",
    "with torch.no_grad():\n",
    "    preds = softMax(net.forward(test_image))\n",
    "\n",
    "color_map = {0:0,1:1/3,2:2/3,3:1}\n",
    "preds = predToSegmentation(preds)\n",
    "seg = torch.zeros((256,256))\n",
    "for i in range(len(preds[0])):\n",
    "    for x in range(256):\n",
    "        for y in range(256):\n",
    "            if preds[0][i][x][y] == 1:\n",
    "                seg[x][y] = color_map[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,15))\n",
    "axs[0].imshow(test_image.squeeze(0)[0],cmap='gray')\n",
    "axs[0].set_title('Image')\n",
    "axs[1].imshow(test_image_label[0],cmap='gray')\n",
    "axs[1].set_title('GT')\n",
    "axs[2].imshow(seg,cmap='gray')\n",
    "axs[2].set_title('prediction mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "HD = computeHD(preds[0][1:],test_image_label[0])\n",
    "HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe190f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DSC = computeDSC(preds[0][1:],test_image_label[0])\n",
    "DSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.load(r'Results\\Statistics\\Tversky_Model_0.3_0.8_att\\TrainLosses.npy')\n",
    "val_loss = np.load(r'Results\\Statistics\\Tversky_Model_0.3_0.8_att\\ValLosses.npy')\n",
    "plt.plot(train_loss,label='train loss')\n",
    "plt.plot(val_loss,label='val loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(train_loss))\n",
    "print(min(val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
